<XLINK:TOC.NFO,TOC> Back to Table of Contents<CON:0D>
<CON:0D>
This will not be an article for teaching assembly or machine language, but is intended to give an orientation of what the relationship between those two terms is.<CON:0D>
<CON:FE>
<CON:0D>
High level languages, such as C and Pascal, both began their development around 1970.  The concept of high level languages did come from earlier work, with FORTRAN, ALGOL, and COBOL.  But before any of this, systems had to be programmed using "machine language."<CON:0D>
<CON:0D>
Machine language refers to actual signal encodings that instruct a computer system to perform an operation. These encodings are typically called "opcodes" (operating code), which are then followed by several "operands" that are like parameters passed to the opcode.  Opcodes often aren't specifically chosen, but are a natural result of how the electronics of the system is built.  As a specific example, a Logical-OR instruction may be implemented as 00110011, that being the digital sequence that the machine interprets as the intent to perform an OR operation.  Hence, why it is called "machine language" since the sequence is specific to the hardware arrangement of that machine.<CON:0D>
<CON:0D>
Between "raw machine code" and "high level languages" sits a concept called "assembly language."  The main aspect of assembly language is that the opcodes of a system are represented by a "mnemonic" instead of the actual digital code of that system.  This instantly made it more effective at expressing the intent of a program (e.g. "OR two values").<CON:0D>
<CON:0D>
Another very useful aspect of assembly language is that various addressing modes can be deduced automatically and symbolic references can be used.  For example, to do a logical OR operation between two registers versus a register and memory location is two different opcodes (since two different addressing modes are involved).  But in assembler, one can still just specify "OR R1,$A123" and the assembler can resolve the necessary opcode automatically.<CON:0D>
<CON:0D>
Symbolic references is most useful in branching - to branch "near" (only a few addresses away) can be done quickly but uses a different opcode.  To branch "far", such as perhaps multiple memory page regions away, may require a different opcode or some initial preperation work.   In either case, programmers just express "BRA  NEXT_STAGE" and the assembler resolves the necessary opcodes depending on the final arrangement of the code.<CON:0D>
<CON:0D>
There is no "standard" assembly language.  While microprocessor designers may note or suggest a particular mnemonic when describing their instruction set, an "assembler" is a piece of software that can use its own conventions.  This is why a system might have multiple assemblers, as each one "competes" on features that may make it more appealing (such as by being faster or easier to use) and each assembler could use slightly different conventions or mnemonics.<CON:0D>
<CON:0D>
As simple example of a typical assembly language, it may look like "ORA $F000" which could mean "OR the current value in the A register with the contents of memory address $F000, and store the result back into register A."  Humans have had alphabetical languages for centuries, and so for us and our training the expression "ORA $F000" is concise and fairly easy to remember.  When an "assembler" converts this assembly into machine code, the result might look like this in binary:  "10110111 11110000"  This can be represented in hex as B7F0h, and could be a specialized opcode where "B" means write to the first byte of a given page, "7" indicates to do an OR operation on Register A, and "F0" is the base page.<CON:0D>
<CON:0D>
This is a completely hypothetical instruction, the 6502 microprocessors has nothing like this.  This just to show the difference between machine code (or object code) and assembler notation, and that a assembler is more than just a straight conversion of mnemonics to opcodes.  But this also relates to the debate on RISC vs CISC: early processors had some fairly complex instructions that would be difficult to implement. Starting in the late 1960s it began to be realized that a more concise reduced instruction set might actually be overall more efficient.<CON:0D>
<CON:0D>
Recap: (notional example)<CON:0D>
<CON:FF>Machine Code       Hex   Assembly Mnemonic<CON:0D>
<CON:FF>10110111 11110000  B7F0  ORA $F000<CON:0D>
<CON:0D>
This is one of the fundamental concepts of software programming and digital computing.  In a nutshell: an electronic clock is used to cycle instructions across a processor.  If the clock is too fast, the processor doesn't have enough time to read and execute the instruction, and "unpredictable" things happen (like running a car engine too fast, it seizes up as inertia of the intake and exhaust valves can't keep up).  It has taken time to engineer these processors to operate at higher clock speeds, to build them smaller and stay cool enough to function at room tempature.  Between about 1978 to 2005, processor clocks went from barely 1MHz on up to 3000MHz, coming close to the theoretical limits of how fast we can switch/cycle instructions through a processor (and so, advances since about 2005 have been on doing more in parallel rather than on raw clock speed; i.e. multi-core).<CON:0D>
<CON:0D>
Modern processor will have sophisticated multi-stage pipelines, that improve performance by processing different aspects of an instruction in parallel, or even predicting results and anticipating the next instruction.  But fundamentally, all processors are based on the above principle: a sequence of bits commands a particular Boolean logic path, which can operate on registers or in memory addresses (or multi-tier cache memory).<CON:0D>
<CON:0D>
Sometimes the terms machine language and assembly language may be used interchangeably, which is not quite accurate, but it is generally understood in context.  For example, when BASIC is used to POKE code into memory, one might say they used assembly in their program.  Since they are likely doing a POKE using hex values that are object code, it is more correct to say they are applying machine code.   But since they most likely used an assembler to come up with the necessary object code, and the hex values are literally just an alternative form of that assembly, most folks can accept either term in that context.<CON:0D>
<CON:0D>
REGISTERS AND BOOLEAN<CON:0D>
---------------------<CON:0D>
Machine code can be intidimating since one needs an understanding of CPU Registers and Boolean Logic.  While this won't be a complete lesson on those topics, as a brief overview:<CON:0D>
<CON:0D>
CPU Registers are essentially your fastest available memory, but most CPUs have very few registers relative to the size of main memory.  At a minimum, like the 6502, there is one "accumulator" register (Register A).  The 6809 has two accumulator registers (A and B) and the Intel 8086 has 8x general purpose registers.  Then there may be a Stack Register (or Stack Pointer), State-Flag register (like for Carry results), and then a Program Counter register.<CON:0D>
<CON:0D>
The main function of a CPU is to perform operations on those registers, like adding values, subtracting values, or Boolean/Bitwise operations.  When one refers to the "bit" of a computer, typically that is the "width" of these internal registers.  Hence, the Commander X16 is an 8-bit computer because its accumulator register is only 8-bit wide.  That is, it can only manipulate or "shovel" 8-bits at a time.  So an 8-bit computer isn't so much "inferior" to a 16 or 32-bit system, it just has a "smaller shovel" of how much work it can do per clock cycle. Thus, generally, an 8-bit system is slower - although there are some situations where (given equal clock speed) an 8-bit system is both faster and technically uses less power.  But even a 1-bit computer can do all the same work, eventually, just as one could use a spoon instead of a shovel.  The original Datapoint 2200 was such a 1-bit computer and it was painfully slow.<CON:0D>
<CON:0D>
Deep inside a CPU is the ALU: Algorithmic Logic Unit.  This is the core component that, for the most part, performs the Boolean logic operations on registers.  Boolean logic refers to terms such as AND, OR, XOR, NOT.  Before the idea of software was ever even imagined, these Boolean operations were realized in eletronics (that has been around since the 1920s).  But step by step, clever engineers built smaller components, and groups of components that could perform Boolean logic.  While to most people it is not at all intuitive on what this could be used for, in the 1940s and 1950s it was realized this could be used for calculation: adding, subtraction, and even floating point.  As you "run out of bits" in your registers, you can store "least significant bits" into a memory, and use a "carry bit" signal on when larger values are involved.  This is severely trivializing the actual sequence of events, but that was the general gist of all the pioneering work in computers from the 1930s to the 1950s: designing hardware components that could represent Boolean logic, and be interconnected to build a system (see Tennis for Two, 1958).<CON:0D>
<CON:0D>
Then early in the 1960s, MIT and the model railroad club were gifted a PDP-1 computer and asked to figure out what it could be used for.  And this is now where more people can start to get an intuitive feel for how this is all useful: model railroad switches.  You can see cases where combinations of switches should impact other actions.  For example, "if this switch AND that switch then flash this signal and close the railroad crossing" (where the switches may correspond to one way rail traffic on a line).  Railroad switching may seem like trivial hobby, but the concept also applies to waterway paths, electrical lines, and even telephone communication lines.<CON:0D>
<CON:0D>
And so from there, computers began to greatly help with telephone signal switching and quickly making decisions on how to route calls.  Examining traffic on a railroad circuit makes it more practical to observe the issues involved (and visualize concepts of half-duplex vs full-duplex when exchanging signals), but generally there are only a few lines to handle in those cases.  Scaling that up to millions of phone connections, computers now had an obvious use beyond just sorting data for insurance and tax records.<CON:0D>
<CON:0D>
Throughout the 1960s, that MIT crew of students pioneered concepts in assembly language and debugging (such as the concept of breakpoints) that would start to be familiar to modern software developers.  But the mathematical principles of Boolean logic, that were evolved in the 40 years prior to that, were still very relevant in writing efficient software.  Great pride went into shaving off extra instructions in hand crafted assembly code, to do things like binary to decimal conversions, integer and floating point conversions, random number generators, etc.<CON:0D>
<CON:0D>
We take such things for granted today, typically since processors are so fast that efficiently written software isn't as paramount.  And modern optimizers are indeed very efficient at probing code several times and finding optimal machine code that captures the intent of high-level languages.  But it is amazing to see during compos or demoscene work, how clever intuition about logic processing makes some amazing result that was not at all obvious until it was revealed.<CON:0D>
<CON:0D>
The difference in well-crafted assembly can be "felt" during the runtime of a game.  It is a different feeling than network lag or latency, but comes down to a responsiveness of a game controller.  This is a feeling that is often lost on many modern systems that old arcade machines have: responses in arcade games were immediate, since the controls are often directly tied to the processor itself, no layers of middle ware in between.   Modern networks and computers are indeed amazing, as we can stream actual controlled gameplay itself across the world.  But there is always still a moment of hesitation that can be felt, the ambiguity of "who moved first," especially as a network gets a hiccup or loses connectivity altogether.<CON:0D>
<CON:0D>
So, that is the background and context of why assembly language still has its appeal.  And there are still many situation where the performance is important enough that "system level developers" will still need to get the job done using assembly language.  Often this is device drivers or "mission critical" software, such as health monitoring and safety systems.  But assembly code is also key to how early microcomputers were able to conduct entertaining games, which would otherwise be impossible with interpreted code.<CON:0D>
<CON:0D>
